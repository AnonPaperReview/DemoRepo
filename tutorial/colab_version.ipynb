{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bf4188ee",
   "metadata": {},
   "source": [
    "Dear Reviewer,\n",
    "\n",
    "This is a short demo script showcasing the testing modules of the IonoBench Framework. (It will also serve as documentation script for intro to IonoBench)\n",
    "\n",
    "You can download the provided dataset and pretrained models, then run the tests to validate the results.\n",
    "\n",
    "The framework is still under active development. It doesn’t yet support a CLI (Next planned action), but you can run each cell step by step with the provided comments.\n",
    "\n",
    "\n",
    "\n",
    "**Instructions**  \n",
    "This notebook is designed for Google Colab with a GPU runtime. Make sure to select a GPU before running.\n",
    "If you’d like to skip the default testing (~20 min on a T4 GPU) or the Solar Analysis (~20 min), feel free to jump directly to the storm evaluation section—it runs much faster.\n",
    "Before that please make sure you run until (`# 3: Testing the trained model >>> Load the model weights from a checkpoint file `)\n",
    "\n",
    "**Common Issues**\n",
    "- **Session crash:** Restart and re-run.\n",
    "- **NVIDIA driver error:** Check if the GPU is correctly shown in the first cell (`# 0: Preps >>> Verify GPU session`).\n",
    "- **Timeout:** Colab sessions are limited; you may need to restart and rerun.\n",
    "- **Download errors:** If dataset or model download fails, delete any existing `datasets/` or `training_sessions/` folders and try again.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f320a3c0",
   "metadata": {},
   "source": [
    "--- \n",
    "## 0: Preps  \n",
    "Clone repo, install requirements, and verify GPU\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47e64371",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 0: Preps >>> Clone Repo\n",
    "#================================================================\n",
    "!git clone https://github.com/AnonPaperReview/DemoRepo.git --quiet\n",
    "%cd DemoRepo\n",
    "#================================================================"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf85600d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 0: Preps >>> Download the required libs\n",
    "#================================================================\n",
    "!pip install -r /content/DemoRepo/requirements.txt --quiet              # Can take couple of minutes\n",
    "!pip install --quiet torch torchvision --index-url https://download.pytorch.org/whl/cu118 # Install cu118 Just incase if pytorch can't find \n",
    "#================================================================"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a50bdefd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 0: Preps >>> Verify GPU session\n",
    "#================================================================\n",
    "import torch\n",
    "print(\"CUDA available:\", torch.cuda.is_available())\n",
    "print(\"Device name:\", torch.cuda.get_device_name(0) if torch.cuda.is_available() else \"No GPU\")\n",
    "#================================================================"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d21724f",
   "metadata": {},
   "source": [
    "--- \n",
    "## 1: Dataset  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f0cc968",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1: Dataset >>> Login to Hugging Face Hub\n",
    "#================================================================\n",
    "from huggingface_hub import login\n",
    "login(\"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\")    # Temporary token for review. Normally users need to create their own token with their account at https://huggingface.co/settings/tokens.\n",
    "# Alternatively, they can download manually once the dataset and models are set to public.\n",
    "#================================================================"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40245fd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1: Dataset >>> Download the desired IonoBench dataset (Stratified or Chronological split)\n",
    "#==================================================================================\n",
    "from pathlib import Path\n",
    "import os,sys\n",
    "\n",
    "repo_name = \"DemoRepo\"  \n",
    "base_path = Path(f\"/content/{repo_name}\")\n",
    "sys.path.append(str(base_path))\n",
    "sys.path.append('./source')\n",
    "sys.path.append('./scripts')\n",
    "from source.myDataFuns import download_dataset\n",
    "download_dataset(\n",
    "    dataset_name=\"stratified\",  # If chronological split is desired, change to \"chronological\"\n",
    "    base_path=base_path\n",
    "    )     \n",
    "#==================================================================================\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3607a9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1: Dataset >>> Read dataset\n",
    "#====================================================================================\n",
    "from scripts.data import load_training_data\n",
    "\n",
    "dataDict = load_training_data(seq_len=12,\n",
    "                            pred_horz=12,\n",
    "                            datasplit='stratified', \n",
    "                            features = None,             # Default \"None\" loads all features, otherwise specify a list of features i.e ['F10.7', 'Dst']\n",
    "                            base_path=base_path)        \n",
    "# You can also see the full list of features in ~/configs/base.yaml\n",
    "\n",
    "print(\"\\n\", \"-\" * 20, \"\\n\", dataDict.keys())\n",
    "print(\"You can use the dataDict following dict_keys as you like from this point on.\")\n",
    "#===================================================================================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a9ab6cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1: Dataset >>> Ex: Accessing specific OMNI features\n",
    "#====================================================================================\n",
    "dataDict['OMNI_names']\n",
    "#====================================================================================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15f5e5c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1: Dataset >>> Ex: Accessing specific OMNI features (note: all features are normalized to 0-1) \n",
    "#=====================================================================================\n",
    "'''\n",
    "\"normOMNI\" is same order with the \"OMNI_names\"\n",
    "Check the name and access as the same idx   (This can be smoother in the future developments)\n",
    "'''\n",
    "# To access 'Dst' values \n",
    "print(\"Dst:\",dataDict['normOMNI'][:,1]) # Dst values\n",
    "\n",
    "# To access 'Year' values\n",
    "print('Year:',dataDict['normOMNI'][:,14]) # Year values\n",
    "#======================================================================================"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "353c1640",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1: Dataset >>> Ex: Accessing TEC data for specific date range (note: tec data is normalized to 0-1)\n",
    "from datetime import datetime, timedelta\n",
    "from source.myDataFuns import dateList\n",
    "# Select the date period\n",
    "startDate = datetime(2024, 5, 9, 0, 0)  # Start date\n",
    "endDate = datetime(2024, 5, 12, 0, 0)    # End date\n",
    "\n",
    "# Find idx corresponding to start and end dates\n",
    "startidx = dataDict['dates'].index(startDate)\n",
    "endidx = dataDict['dates'].index(endDate)\n",
    "\n",
    "# Extract TEC data for the selected date range\n",
    "norm_tecData = dataDict['normTEC'][startidx:endidx+1]\n",
    "date_list = dateList(startDate, endDate,timedelta(hours=2)) # TEC maps are 2 hours apart\n",
    "print(\"TEC data shape for the selected date range:\", norm_tecData.shape)\n",
    "print(\"Dates of the selected date range:\", len(date_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3885239b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1: Dataset >>> TO DO: Add here a animation script to visualize the TEC data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de9b00ee",
   "metadata": {},
   "source": [
    "--- \n",
    "## 2: Models  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a8601b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2: Models and Configs >>> Download the desired Trained Model (SimVP2,SwinLSTM, DCNN etc.) from Hugging Face Hub\n",
    "#==================================================================================\n",
    "from pathlib import Path\n",
    "\n",
    "from source.myDataFuns import download_model_folder\n",
    "\n",
    "data_path = Path(base_path, \"training_sessions\")         # ..~/DemoRepo/training_sessions \n",
    "data_path.mkdir(parents=True, exist_ok=True)             # Create new folder when model folder is downloaded.\n",
    "\n",
    "                          #\n",
    "download_model_folder(                                  # This function automatically downloads the paper's model folder from Hugging Face Hub.\n",
    "                    model_name = \"SimVPv2\",              # Change to \"DCNN\", \"SwinLSTM\", \"SimVPv2_Chrono\" for other models.\n",
    "                    base_path = base_path)    \n",
    "#==================================================================================\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9da2d08",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2: Models and Configs >>> Load the model configurations\n",
    "#==================================================================================\n",
    "from scripts.loadConfigs import load_configs\n",
    "\n",
    "'''\n",
    "Configs uses Base => model => mode => CLI overrides hierarchy.\n",
    "You can change the model and mode to load different configurations.\n",
    "For example, change \"SimVPv2\" to \"DCNN\", \"SwinLSTM\" etc. to load different model configurations.\n",
    "load_configs will return a merged configurations of the base, model, and mode.\n",
    "'''\n",
    "\n",
    "cfgs = load_configs(\n",
    "                 model = \"SimVPv2\",  # Change to \"DCNN\", \"SwinLSTM\", \"SimVPv2_Chrono\" for other models.\n",
    "                 mode = \"test\",\n",
    "                 base_path= base_path\n",
    "                 )\n",
    "#=================================================================================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6ceca61",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2: Models and Configs >>> Can check the keys of the config dictionary\n",
    "#================================================================\n",
    "print(cfgs.keys())\n",
    "print(cfgs.data.keys())\n",
    "print(cfgs.model.keys())\n",
    "print(cfgs.data.data_split)\n",
    "#================================================================"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9795a207",
   "metadata": {},
   "source": [
    "--- \n",
    "## 3: Testing Trained Models \n",
    "--- "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b75fa3bb",
   "metadata": {},
   "source": [
    "\n",
    "### 3a: Loading Pre-trained Model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "021206b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3a: Loading Pre-trained Model>>> Prepare the data and build the model     \n",
    "#=================================================================\n",
    "\n",
    "from scripts.registry import build_model\n",
    "from scripts.data import prepare_raw\n",
    "from torchinfo import summary\n",
    "\n",
    "# Prepare the data for testing\n",
    "cfgs.test.batch_size = 64                           # Lower batch size for testing\n",
    "data = prepare_raw(cfgs)               # This fun Wraps load_training_data & patches cfg with shape/min-max info.\n",
    "B = cfgs.test.batch_size               # batch size from YAML\n",
    "T = cfgs.data.seq_len                  # input sequence length\n",
    "C = cfgs.data.num_omni + 1             # OMNI scalars + TEC map channel\n",
    "H = cfgs.data.H                        # height (set inside prepare_raw)\n",
    "W = cfgs.data.W                        # width  (set inside prepare_raw)\n",
    "cfgs.test.input_names = data['OMNI_names']          # This sets the input names for test logging file\n",
    "\n",
    "'''\n",
    "Note: The original model was trained with a batch size of 128 in the developer environment.\n",
    "Due to resource limitations on free Google Colab, the batch size has been reduced to 64 for testing.\n",
    "You should adjust the batch size accordingly based on the specific model. The summary function will print useful information to help set an appropriate batch size for testing.\n",
    "'''\n",
    "\n",
    "device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"  # Use GPU if available, otherwise CPU\n",
    "# Build the model\n",
    "model = build_model(cfg = cfgs, base_path=base_path, device=device)           # Build the model with the configurations\n",
    "cfgs.model.input_shape = (T, C, H, W)  # Set input shape for the model\n",
    "summary(model, input_size=((B, C, T, H, W),(B, cfgs.data.pred_horz, H, W)))\n",
    "print(f\"Batch size: {B}, T: {T}, C: {C}, H: {H}, W: {W}\")\n",
    "#=================================================================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aeb1b869",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3a: Loading Pre-trained Model >>> Summary of the model   \n",
    "#=================================================================\n",
    "summary(model, input_size=((B, C, T, H, W),(B, cfgs.data.pred_horz, H, W)))\n",
    "#================================================================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "558ace4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3a: Loading Pre-trained Model >>> Load the model weights from a checkpoint file\n",
    "#====================================================================================\n",
    "from source.myTrainFuns import DDPtoSingleGPU\n",
    "import torch\n",
    "\n",
    "'''\n",
    "!!!! Change the Path of **torch.load** to the path of your downloaded model checkpoint file  !!!!\n",
    "You can find the checkpoint file in the downloaded model folder under `training_sessions/{modelName}` as .... \"NameofTheSession\"_best_checkpoint_\"yyyymmdd\"_\"hhmm\"\n",
    "'''\n",
    "checkpoint = torch.load(\n",
    "    r'/content/DemoRepo/training_sessions/SimVPv2/SimVP_stratifiedSplit_Allfeatures_best_checkpoint_20250320_1401.pth',            # <= Copy inside\n",
    "    weights_only=True)\n",
    "model.load_state_dict(DDPtoSingleGPU(checkpoint[\"model_state_dict\"]))   # Load the model state dict If DDP(Multiple GPU) was used get rid of Module prefix\n",
    "#===================================================================================="
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18913f21",
   "metadata": {},
   "source": [
    "--- \n",
    "### 3b: Default Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f35c1d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3b: Default Test >>> Setup the data loaders for testing\n",
    "from scripts.data import make_default_loaders\n",
    "loaders = make_default_loaders(cfg = cfgs, d = data)                # Make default loaders for training, validation, and test sets.\n",
    "len(loaders[\"train\"]), len(loaders[\"valid\"]), len(loaders[\"test\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f3b9a33",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3b: Default Test >>> Test the model (Default: test set) (RUN TIME: >~20 min)\n",
    "from source.myTrainFuns import IonoTester\n",
    "\n",
    "cfgs.test.save_results = True           # Save the test results to a log file (txt).\n",
    "cfgs.test.save_raw = False              # Save the raw test results (Prediction Dates, TEC predictions and dedicated truth maps) to npz.\n",
    "cfgs.session.name = \"SimVPv2_test\"      # Write a session name for the test results. New name will create a new folder in the ~/IonoBenchv1/training_sessions/\n",
    "testDict = IonoTester(model, loaders['test'], device=device, config=cfgs).test() \n",
    "'''\n",
    "!!! Disclaimer !!!: \n",
    "The paper's default experiments (excluding solar and storm cases) were trained and tested using 4 GPUs with Distributed Data Parallel (DDP).\n",
    "DDP introduces some non-determinism even with fixed random seeds due to differences in data shuffling and floating-point operation order across GPUs.    \n",
    "\n",
    "This notebook uses a single GPU for testing, which may lead to slight differences in results.\n",
    "\n",
    "These differences are typically negligible, as can be seen by comparing with the paper's Table 2 values. Reproducing the *exact* model weights from the \n",
    "paper's DDP training run can be challenging; the key is that different DDP training runs \n",
    "yields models with very similar performance characteristics.\n",
    "\n",
    "For consistent reproduction of your result ensure a single GPU environment and fixed number of GPUs for the model experiments.\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cba3a0a",
   "metadata": {},
   "source": [
    "--- \n",
    "## 4: Solar and Storm Analysis\n",
    "---\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f08ca93",
   "metadata": {},
   "source": [
    "\n",
    "### 4a: Solar Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88217c4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4a: Solar and Storm Analysis >>> Solar Loaders \n",
    "#====================================================================================\n",
    "from scripts.data import make_solar_loaders\n",
    "\n",
    "if \"loaders\" in locals():\n",
    "    del loaders, data, cfgs             # To clear memory before loading solar loaders\n",
    "    \n",
    "import gc\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "# Change mode to \"solar\" in the configs to load solar intensity dataloaders.\n",
    "cfgs = load_configs(\n",
    "                 model = \"SimVPv2\", \n",
    "                 mode = \"solar\",     # <= change here to dynamically change the testing mode\n",
    "                 base_path= base_path\n",
    "                 )\n",
    "cfgs.paths.base_dir = base_path\n",
    "data = prepare_raw(cfgs)                # Prepare the data (again neeeded for setting important parameters to config inside) \n",
    "loaders = make_solar_loaders(cfg = cfgs, base_path=base_path, d = data)                # Make default loaders for training, validation, and test sets.\n",
    "#===================================================================================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ebc9a94",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4a: Solar Analysis >>> Analysis Function (Testing on Solar intensity classes: very weak, weak, moderate, intense) (RUN TIME: >~20 Mins)\n",
    "#====================================================================================\n",
    "from source.myTrainFuns import SolarAnalysis\n",
    "cfgs.session.name = \"SimVPv2_test\"  # Writes on top of the previous test file if the session name is the same.\n",
    "cfgs.test.save_results = True       # Appends the test results to a log file (txt) that exists in the training_sessions folder under the session name. (if not exists, creates a new one) \n",
    "cfgs.test.save_raw = False          # Save the raw test results (Prediction Dates, TEC predictions and dedicated truth maps) to npz. (Per solar class)\n",
    "solarDict = SolarAnalysis(model, data, loaders, device=device, cfg=cfgs).run()\n",
    "#===================================================================================="
   ]
  },
  {
   "cell_type": "markdown",
   "id": "820351b4",
   "metadata": {},
   "source": [
    "---\n",
    "### 4b: Storm Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55aa438e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4b: Storm Analysis >>> Storm Loaders\n",
    "#====================================================================================\n",
    "from scripts.data import make_storm_loaders\n",
    "\n",
    "if \"loaders\" in locals():\n",
    "    del loaders, data, cfgs             # To clear memory before loading solar loaders\n",
    "    \n",
    "import gc\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "# Change mode to \"solar\" in the configs to load solar intensity dataloaders.\n",
    "cfgs = load_configs(\n",
    "                 model = \"SimVPv2\", \n",
    "                 mode = \"storm\",     # <= change here to dynamically change the testing mode\n",
    "                 base_path= base_path\n",
    "                 )\n",
    "cfgs.paths.base_dir = base_path\n",
    "data = prepare_raw(cfgs)                # Prepare the data (again neeeded for setting important parameters to config inside) \n",
    "loaders = make_storm_loaders(cfg = cfgs, d = data)                # Make default loaders for training, validation, and test sets.\n",
    "#===================================================================================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00951699",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4b: Storm Analysis >>> Analysis Function (Testing on Storm events) (RUN TIME: ~1-3 Mins)\n",
    "#====================================================================================\n",
    "from source.myTrainFuns import StormAnalysis\n",
    "\n",
    "cfgs.test.save_results = True           # Save the test results to a log file (txt).\n",
    "cfgs.test.save_raw = True               # Save the raw test results (Prediction Dates, TEC predictions and dedicated truth maps) to npz.\n",
    "cfgs.session.name = \"SimVPv2_test\"      # Write a session name for the test results. New name will create a new folder in the ~/IonoBenchv1/training_sessions/\n",
    "testDict = StormAnalysis(model, data, cfgs, loaders,device).run() \n",
    "#===================================================================================="
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
